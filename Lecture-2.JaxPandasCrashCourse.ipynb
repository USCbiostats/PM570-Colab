{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhod1n3RRlTznysyxxr+zn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/USCbiostats/PM570-Colab/blob/main/Lecture-2.JaxPandasCrashCourse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JAX and JAX.Numpy\n",
        "`jax` is a Google-backed library to enable automatic differentiation of Python code, while supporting ultra-fast runtime due to \"Just-In-Time\" (i.e. JIT) compilation from their custome bytecode (i.e. XLA). Hence JAX = **J**IT + **A**utoDiff + **X**LA. \n",
        "\n",
        "Let's practice importing JAX and using the `numpy` implementation backed by JAX. `numpy` is a Python library for n-dimensional arrays. Here we are using JAX's implementation, which will enable us to take advantage of all of JAX's features."
      ],
      "metadata": {
        "id": "7z3qkrvCV3GS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "araRojk1ruYL",
        "outputId": "8fbe175a-30de-4d7e-b807-43b2b2cff343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.lib.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = [0 1 2 3 4 5 6 7 8] | y = [1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "z = [1. 2. 3. 4. 5. 6. 7. 8. 9.] | x + 1 = [1 2 3 4 5 6 7 8 9]\n",
            "i = [[1. 0. 0. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]] | a = [2. 2. 2. 2.]\n",
            "b = [[2. 0. 0. 0.]\n",
            " [0. 2. 0. 0.]\n",
            " [0. 0. 2. 0.]\n",
            " [0. 0. 0. 2.]]\n",
            "A = [[5. 1.]\n",
            " [1. 5.]]\n",
            "b = [[10.  2.]\n",
            " [ 2. 10.]]\n",
            "b = [12. 12.]\n",
            "b = [12. 12.]\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as rdm\n",
        "\n",
        "# readr::read_tsv\n",
        "# let's practice some numpy tricks\n",
        "x = jnp.arange(9)\n",
        "y = jnp.ones(9)\n",
        "print(f\"x = {x} | y = {y}\")\n",
        "\n",
        "z = x + y\n",
        "print(f\"z = {z} | x + 1 = {x + 1}\")\n",
        "\n",
        "P = 4\n",
        "i = jnp.eye(P)\n",
        "a = 2 * jnp.ones(P)\n",
        "print(f\"i = {i} | a = {a}\")\n",
        "\n",
        "# is this mat/vec mult?\n",
        "b = i * a\n",
        "print(f\"b = {b}\")\n",
        "\n",
        "A = jnp.array([[5., 1], [1, 5]])\n",
        "a = 2 * jnp.ones(2)\n",
        "print(f\"A = {A}\")\n",
        "b = A * a\n",
        "print(f\"b = {b}\")\n",
        "\n",
        "# nope! b is matrix; mat/vec mult => vec\n",
        "b = A @ a\n",
        "print(f\"b = {b}\")\n",
        "b = jnp.dot(A, a)\n",
        "print(f\"b = {b}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random variables in JAX\n",
        "key = rdm.PRNGKey(0)\n",
        "\n",
        "key, y_key = rdm.split(key)\n",
        "N = 5\n",
        "mu_y = 50\n",
        "std_y = 100\n",
        "y = mu_y + std_y * rdm.normal(y_key, shape=(N,)) # y_i ~ N(mu_y, std_y)\n",
        "print(f\"y = {y}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGJ19sWvvBUw",
        "outputId": "2eff679c-a25f-4d78-c8f9-8290df3755ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = [ -95.8194  -154.7044   254.73392  166.84094  -47.58364]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.scipy.stats as stats\n",
        "\n",
        "N = 500_000\n",
        "\n",
        "key = rdm.PRNGKey(0)\n",
        "\n",
        "# simulate genotype\n",
        "freq = 0.1\n",
        "key, h1_key, h2_key = rdm.split(key, 3)\n",
        "h1 = rdm.bernoulli(h1_key, freq, shape=(N,)).astype(int) \n",
        "h2 = rdm.bernoulli(h2_key, freq, shape=(N,)).astype(int) \n",
        "x = h1 + h2\n",
        "x = x - 2 * freq\n",
        "\n",
        "# simulate phenotype as a function of genotype\n",
        "h2g = 1e-4\n",
        "key, b_key = rdm.split(key)\n",
        "beta = rdm.normal(b_key)\n",
        "g = x * beta\n",
        "s2g = jnp.var(g)\n",
        "s2e = ((1 / h2g) - 1) * s2g # h2g = s2g / (s2g + s2e) => \n",
        "\n",
        "# phenotype = genetic component + env noise\n",
        "key, y_key = rdm.split(key)\n",
        "y = g + jnp.sqrt(s2e) * rdm.normal(y_key, shape=(N,))\n",
        "y = y - jnp.mean(y)\n",
        "\n",
        "#print(f\"y = {y}\")\n",
        "\n",
        "beta_hat =  (x.T @ y) / (x.T @ x) # (x.T x)^-1 x.T y\n",
        "s2e_hat = jnp.mean((y - x * beta_hat) ** 2)\n",
        "se_beta = jnp.sqrt(s2e_hat / (x.T @ x))\n",
        "\n",
        "print(f\" beta = {beta} | hat(beta) = {beta_hat} | se(hat(beta)) = {se_beta}\")\n",
        "z = beta_hat / se_beta\n",
        "print(f\"z = {z} | p-value = {2*stats.norm.cdf(-jnp.fabs(z))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEodk7fH0hfs",
        "outputId": "fae010ce-b26c-45f9-fef4-27d03524f83f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " beta = -0.1399441361427307 | hat(beta) = -0.1588948369026184 | se(hat(beta)) = 0.019815918058156967\n",
            "z = -8.018545150756836 | p-value = 1.0700497342445428e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LAX and functional design patterns\n",
        "\n",
        "`jax.lax.scan` provides a means to scan along an axis of `ndarray` while carrying state along with it. The psuedocode for `scan` looks like, \n",
        "```python\n",
        "def scan(func, init, xs, length=None):\n",
        "  if xs is None:\n",
        "    xs = [None] * length\n",
        "  carry = init\n",
        "  ys = []\n",
        "  for x in xs:\n",
        "    carry, y = func(carry, x)\n",
        "    ys.append(y)\n",
        "  return carry, np.stack(ys)\n",
        "  ```\n",
        "\n",
        "  Let's see how this can be useful for GWAS..."
      ],
      "metadata": {
        "id": "cuSTu-2pRQEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.lax as lax\n",
        "\n",
        "# simulate geno + pheno\n",
        "N = 10_000\n",
        "P = 10000\n",
        "\n",
        "key = rdm.PRNGKey(0)\n",
        "\n",
        "# simulate genotype\n",
        "freq = 0.1\n",
        "key, h1_key, h2_key = rdm.split(key, 3)\n",
        "h1 = rdm.bernoulli(h1_key, freq, shape=(N,P)).astype(int) \n",
        "h2 = rdm.bernoulli(h2_key, freq, shape=(N,P)).astype(int) \n",
        "X = h1 + h2\n",
        "X = X - 2 * freq\n",
        "\n",
        "# simulate phenotype as a function of genotype\n",
        "h2g = 0.3\n",
        "key, b_key = rdm.split(key)\n",
        "beta = rdm.normal(b_key, shape=(P,))\n",
        "G = X @ beta\n",
        "s2g = jnp.var(G)\n",
        "s2e = ((1 / h2g) - 1) * s2g # h2g = s2g / (s2g + s2e) => \n",
        "\n",
        "# phenotype = genetic component + env noise\n",
        "key, y_key = rdm.split(key)\n",
        "y = G + jnp.sqrt(s2e) * rdm.normal(y_key, shape=(N,))\n",
        "y = y - jnp.mean(y)\n",
        "\n",
        "# lets write a function that performs OLS between a single variant and y, and\n",
        "# reports the effect size estimate, its SE, and pvalue\n",
        "\n",
        "# scan results a 'carry' that keeps state going forward\n",
        "# here we dont require changing state. all we need to\n",
        "# do is keep passing phenotype along\n",
        "def ols_scan(y, x):\n",
        "  xtx = x.T @ x\n",
        "  beta_hat =  (x.T @ y) / (xtx) # (x.T x)^-1 x.T y\n",
        "  s2e_hat = jnp.mean((y - x * beta_hat) ** 2)\n",
        "  se_beta = jnp.sqrt(s2e_hat / (xtx))\n",
        "  p_val = 2*stats.norm.cdf(-jnp.fabs(beta_hat / se_beta))\n",
        "\n",
        "  # scan requires we return updated state (i.e. same y)\n",
        "  # along with the result\n",
        "  return y, jnp.array([beta_hat, se_beta, p_val])\n",
        "\n",
        "_, gwas_res = lax.scan(ols_scan, y, X.T)\n",
        "\n",
        "# print first 5 results...\n",
        "print(\"BETA\\tSE\\tPVal\")\n",
        "print(gwas_res[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNHIFyDXRPHK",
        "outputId": "c79f1250-d9e6-4009-f6a0-009212878c45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BETA\tSE\tPVal\n",
            "[[-0.89536387  1.8326901   0.6251591 ]\n",
            " [ 0.8770723   1.8177588   0.62944937]\n",
            " [ 2.8826997   1.8519931   0.11957997]\n",
            " [ 1.3135461   1.8424423   0.47588444]\n",
            " [ 0.20249538  1.8436264   0.91253996]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Just-in-time compilation and function decorators\n",
        "Let's see if we can use JIT to speed up our GWAS scan. To do that we'll need to review adding \"decorators\" to python functions that modify them in some way."
      ],
      "metadata": {
        "id": "CJ45QVJCuq1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax._src.api import block_until_ready\n",
        "# JIT warm up\n",
        "\n",
        "def my_func(x):\n",
        "  return jnp.sum(x ** 2)\n",
        "\n",
        "# `jax.jit` takes as input a function and returns the JIT-compiled function\n",
        "my_func_jit = jax.jit(my_func)\n",
        "\n",
        "# results should be the same\n",
        "is_same = jnp.allclose(my_func(jnp.ones(4)), my_func_jit(jnp.ones(4)))\n",
        "print(f\"Results are same? {is_same}\")\n",
        "\n",
        "%timeit my_func(jnp.ones(4)) # let's measure time\n",
        "%timeit my_func_jit(jnp.ones(4)).block_until_ready() # measure using JIT; need to block until result is returned\n",
        "\n",
        "# results computed faster in the JIT compiled function! We did no extra work\n",
        "# except wrap our function using a JAX command! Now let's see how to \n",
        "# use the decorator sytax to handle that automatically for us\n",
        "\n",
        "@jax.jit\n",
        "def my_new_func(x):\n",
        "  return jnp.sum(x ** 2)\n",
        "\n",
        "# the @jax.jit above the function definition informs the Python interpreter\n",
        "# to \"decorate\" `my_new_func` with the `jax.jit` function, which will automatically\n",
        "# wrap my_new_func in the JIT compiled version. That is, anytime we call `my_new_func`\n",
        "# we're actually calling the same thing as `jax.jit(my_new_func)`\n",
        "%timeit my_new_func(jnp.ones(4)).block_until_ready()\n",
        "\n",
        "# the average time is similar to the above `my_func_jit` which shows that we're\n",
        "# calling the JIT'd version. "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gChVtQ8Qups8",
        "outputId": "af4d97db-45be-4c32-b732-4602b88e2b68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results are same? True\n",
            "620 µs ± 81.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "309 µs ± 9.42 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
            "312 µs ± 6.77 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's apply this to our GWAS scan. We'll skip\n",
        "# the decorator syntax for now to illustrate the speedup for our scan.\n",
        "\n",
        "def gwas_scan_slow(X, y):\n",
        "  _, gwas_res = lax.scan(ols_scan, y, X.T)\n",
        "  return gwas_res\n",
        "\n",
        "gwas_scan_fast = jax.jit(gwas_scan_slow)\n",
        "%timeit gwas_scan_slow(X, y).block_until_ready()\n",
        "%timeit gwas_scan_fast(X, y).block_until_ready()\n",
        "\n",
        "# the speedup here seems marginal, but this only will improve as sample\n",
        "# sizes get bigger and our functions are more complex!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Moh-lRLxxQc",
        "outputId": "7510a0af-1421-4de8-9045-02fec578b01e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.51 s ± 75.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "2.48 s ± 74.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pandas\n",
        "Pandas is a Python library for datatable/dataframe like data structures.\n",
        "Let's take our GWAS results and convert to dataframe for easier manipulation"
      ],
      "metadata": {
        "id": "_iQkLh-CVFsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_gwas = pd.DataFrame(gwas_res, columns=[\"BETA\", \"SE\", \"PVal\"])\n",
        "print(df_gwas.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne6nAl9qVOk7",
        "outputId": "acafb40f-bee2-4732-ca2b-9a1b0f587d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         BETA         SE        PVal\n",
            "0  -11.807892   31.55952   0.7082951\n",
            "1    8.070707  31.307985   0.7965734\n",
            "2   24.326462   31.32242  0.43736708\n",
            "3   -0.281418   32.09645   0.9930043\n",
            "4   14.883338  31.868835  0.64048654\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VMAP, or how I learned to stop worrying and love vectorization\n",
        "Say we have a function defined only for a particular shape of data, and we would like to write a similarly but for multiple \"batches\" of data. How could we do that?\n",
        "\n",
        "In comes `jax.vmap`."
      ],
      "metadata": {
        "id": "9pnPoSfvqjoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ols_single(y: jnp.ndarray, x: jnp.ndarray) -> jnp.ndarray:\n",
        "  xtx = x.T @ x\n",
        "  beta_hat =  (x.T @ y) / (xtx) # (x.T x)^-1 x.T y\n",
        "  s2e_hat = jnp.mean((y - x * beta_hat) ** 2)\n",
        "  se_beta = jnp.sqrt(s2e_hat / (xtx))\n",
        "  p_val = 2*stats.norm.cdf(-jnp.fabs(beta_hat / se_beta))\n",
        "\n",
        "  # scan requires we return updated state (i.e. same y)\n",
        "  # along with the result\n",
        "  return jnp.array([beta_hat, se_beta, p_val])\n",
        "\n",
        "results = []\n",
        "for i in range(10):\n",
        "  results.append(ols_single(y, X.T[i]))\n",
        "results = jnp.array(results)\n",
        "print(f\"Results = {results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgowJjOXqVoQ",
        "outputId": "72cf0862-ef73-4bc8-b851-601191949295"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results = [[-0.89536387  1.8326901   0.6251591 ]\n",
            " [ 0.8770723   1.8177588   0.62944937]\n",
            " [ 2.8826997   1.8519931   0.11957997]\n",
            " [ 1.3135461   1.8424423   0.47588444]\n",
            " [ 0.20249538  1.8436264   0.91253996]\n",
            " [-2.5835547   1.8322238   0.15852053]\n",
            " [-2.4758806   1.8603319   0.18322818]\n",
            " [ 0.33134592  1.8274202   0.85611725]\n",
            " [-0.9843171   1.8259833   0.58984447]\n",
            " [-0.7290425   1.8216625   0.68900394]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#subset first 10 SNPs\n",
        "X_sub = X[:,:10]\n",
        "\n",
        "# call vmap on our ols function, but vectorize only along the genotype\n",
        "# hence \"(None, 0)\", and vectorize output along genotype axis \"0\"\n",
        "ols_multi = jax.vmap(ols_single, (None, 0), 0)\n",
        "results = ols_multi(y, X_sub.T)\n",
        "print(f\"Results = {results}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6B5hPd6ir-MB",
        "outputId": "4829d08c-db3e-400e-ea9e-0711b2f1f120"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results = [[-0.8953643   1.8326901   0.62515897]\n",
            " [ 0.8770692   1.8177588   0.62945056]\n",
            " [ 2.882697    1.8519931   0.11958028]\n",
            " [ 1.3135438   1.8424423   0.47588527]\n",
            " [ 0.20249653  1.8436264   0.9125395 ]\n",
            " [-2.5835583   1.8322238   0.15851992]\n",
            " [-2.4758773   1.8603319   0.18322876]\n",
            " [ 0.33134153  1.8274201   0.8561191 ]\n",
            " [-0.9843188   1.8259833   0.58984387]\n",
            " [-0.72904265  1.8216625   0.6890038 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can combine this with JIT!\n",
        "ols_multi_fast = jax.jit(ols_multi)\n",
        "\n",
        "%timeit ols_multi(y, X_sub.T).block_until_ready()\n",
        "%timeit ols_multi_fast(y, X_sub.T).block_until_ready()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya4gNxdIszAK",
        "outputId": "55aa468c-31e3-4b01-ec27-0952fd77f020"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26.7 ms ± 545 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "449 µs ± 9.12 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd\n",
        "Automatic differentiation is a field of study that focuses on how to compute derivatives of computer code algorithmically"
      ],
      "metadata": {
        "id": "GtRZd4i9u70m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sos(x: jnp.ndarray) -> float:\n",
        "  return jnp.sum(x ** 2)\n",
        "\n",
        "x = 2 * jnp.ones(5)\n",
        "sos(x)\n",
        "\n",
        "def sos_handcoded_deriv(x: jnp.ndarray) -> jnp.ndarray:\n",
        "  return 2 * x\n",
        "\n",
        "# we can just `jax.grad` to get the gradient function automatically\n",
        "sos_prime = jax.grad(sos)\n",
        "print(f\"Are derivatives the same? {jnp.allclose(sos_prime(x), sos_handcoded_deriv(x))}\")\n",
        "\n",
        "sos_hess = jax.hessian(sos)\n",
        "print(f\"Hessian = {sos_hess(x)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-eIkGL5vPkH",
        "outputId": "b3e3bda5-81a4-4b9c-b64b-7f57d309b48a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are derivatives the same? True\n",
            "Hessian = [[2. 0. 0. 0. 0.]\n",
            " [0. 2. 0. 0. 0.]\n",
            " [0. 0. 2. 0. 0.]\n",
            " [0. 0. 0. 2. 0.]\n",
            " [0. 0. 0. 0. 2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inv(A) @ v; we are solving for direction d in Ad = v\n",
        "# the solution d = inv(A) @ v is the same solution to following optimization problem\n",
        "# min_d 0.5*d.T @ A @ d + d.T @ v\n",
        "\n",
        "# TODO: later"
      ],
      "metadata": {
        "id": "m93JiJJlxEOS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}